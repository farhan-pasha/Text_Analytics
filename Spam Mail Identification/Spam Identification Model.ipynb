{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Import the required packages</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import nltk\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> We first write a function to preprocess the data which does below:</b> <br>\n",
    "1) We first define some unwanted characters. <br>\n",
    "2) Then we tokenize the data.<br>\n",
    "3) Then we remove the stop words and unwanted characters that we defined and stem the final output.<br>\n",
    "4) then we join the tokens together to form a string and return it.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "unwanted = ['\\n','\\t','\\r','\\'','\"',':',';','.','/','!','#','$','%','{','}','(',')','?','&',']','[','-','_']\n",
    "\n",
    "\n",
    "def pre(text1):\n",
    " lis1=nltk.word_tokenize(text1)\n",
    " lis1=[nltk.stem.PorterStemmer().stem(word.lower()) for word in lis1 if word not in nltk.corpus.stopwords.words(\"English\") and word not in unwanted]\n",
    " text=\" \".join(lis1)\n",
    " return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Then using the preprocessing function above we append preprocessed files to respective corpus: file_ham and file_spam <br>\n",
    "2) We also create a list with label for each item in corpus in the name of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ham=[]\n",
    "for filename in glob.glob(os.path.join(\"./MailDataset/ham\", '*.txt')):\n",
    " file=open(filename,'r')\n",
    " file_ham.append(pre(file.read()))\n",
    "\n",
    "\n",
    "file_spam=[]\n",
    "for filename in glob.glob(os.path.join(\"./MailDataset/spam\", '*.txt')):\n",
    " file=open(filename,'r')\n",
    " file_spam.append(pre(file.read()))\n",
    "\n",
    "\n",
    "labels=[]\n",
    "\n",
    "for i in range(0,200):\n",
    "    if i<100:\n",
    "        labels.append(\"ham\")\n",
    "    else:\n",
    "        labels.append(\"spam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Using the above dataset we <b>train</b> the model with <b>60% data</b><br>\n",
    "2) We first convert the data into tfidf vectors.<br>\n",
    "3) Then we define the svm model and fit training data to it with different values of hyperparameters and finally use  gamma=.001, C=4 after multiple performance tests.<br>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=4, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus=file_ham[40:100]+file_spam[:60]\n",
    "print(len(corpus))\n",
    "count_vect=CountVectorizer()\n",
    "tfidf_transformer=TfidfTransformer()\n",
    "counts=count_vect.fit_transform(corpus)\n",
    "Z=tfidf_transformer.fit_transform(counts)\n",
    "\n",
    "model=svm.SVC(gamma=.001, C=4)\n",
    "model.fit(Z,labels[:60]+labels[140:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Then we use 40% data to test the model and use accuracy as the metric to measure the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8625\n"
     ]
    }
   ],
   "source": [
    "\n",
    "counter=count_vect.transform(file_ham[:40]+file_spam[60:100])\n",
    "predict=tfidf_transformer.transform(counter)\n",
    "\n",
    "\n",
    "model.predict(predict)\n",
    "predicted=model.predict(predict)\n",
    "print(accuracy_score(predicted,(labels[60:100]+labels[100:140])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above the accuracy is 86.25% which is good considering the data that we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
